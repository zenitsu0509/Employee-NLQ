database:
  connection_string: ${DATABASE_URL}
  pool_size: 10
  pool_timeout: 30
embeddings:
  model: sentence-transformers/all-MiniLM-L6-v2
  batch_size: 32
  device: cpu
cache:
  ttl_seconds: 300
  max_size: 1000
  backend: redis
logging:
  level: INFO
  retention_days: 7
# Groq LLM settings
groq:
  # provider can be 'groq' or 'openai'
  provider: groq
  # Use your Groq API key from environment
  api_key: "${GROQ_API_KEY}"
  
  model: "gpt-oss-120b"

# Background queue (Redis + RQ) configuration
queue:
  enabled: false
  redis_url: redis://localhost:6379/0
  queue_name: ingestion

# Vector store configuration
vector_store:
  # faiss (in-memory) or pgvector
  type: faiss
  # Only for pgvector; will default to DATABASE_URL if omitted
  connection_string: ${VECTOR_DB_URL}
  table_name: document_chunks
